---
title: "DSE 6311 Hyperparameter Tuning & Model Evaluation"
author: "James Keegan"
date: "2025-04-24"
output: html_document
---

# Code Copied from Steven Martinez

############## Loading Libraries Function ####################

```{R}
load_libraries <- function(packages) {
  # Check for missing packages
  missing_packages <- packages[!(packages %in% installed.packages()[, "Package"])]
  
  # install missing packages
  if(length(missing_packages) > 0) {
    install.packages(missing_packages)
  }
  
  # Load all packages
  lapply(packages, library, character.only = TRUE)
  
  cat("All packages are loaded succesfully.\n")
}

# Loading necessary libraries
load_libraries(c("tidyverse", "lubridate", "stats", "ggplot2", "corrplot", "stringr", "stringi", "class", "tidymodels", "modeldata", "themis", "vip", "baguette", "janitor", "rvest", "yardstick", "gsheet", "caret", "randomForest", "here", "tibble", "dplyr", "ISAR", "tidyr", "mgcv", "teamcolors", "baseballr", "Lahman", "remotes", "ggcorrplot", "broom", "readr", "glmnet", "xgboost", "Matrix", "Metrics", "reshape2", "DMwR2", "splitstackshape"))

# Load only the necessary functions from 'car'
library(car, exclude = "select")

# Turning off warning messages
options(warnings = 0)

```

# Loading Dataset

```{R}
# Loading dataset
df <- read_csv("C:\\Users\\student\\Documents\\GitHub\\team-beta\\final_data\\balanced_rf_data.csv")
df <- df %>% rename_with(make.names)
df$TeamSuccess <- as.factor(df$TeamSuccess)

head(df)

```

```{R}
# My job for this week
# rf data knn model
# cross-validation: stratfied k-fold

```

# Modeling

```{R}
set.seed(123)

# Stratified 10-fold CV
ctrl <- trainControl(
  method = "cv",         # cross-validation
  number = 10,           # 10 folds
  # sampling = SMOTE,
  savePredictions = "final"  # keep predictions for evaluation
)

```

```{R}
# KNN Model
knn_model <- train(
  TeamSuccess ~ ., 
  data = df,
  method = "knn",          # K-Nearest Neighbors
  trControl = ctrl,
  tuneLength = 10,         # tests 10 different k values
  metric = "Accuracy"      # or "Kappa"
)

```

```{R}
# Predictions
pred <- knn_model$pred$pred
actual <- knn_model$pred$obs

# Ensure factor levels match (just in case)
pred <- factor(pred, levels = levels(actual))

# Confusion matrix and stats
confusionMatrix(pred, actual)

```

# With Help from Joe

```{R}
# Load Data
data <- read.csv("C:\\Users\\student\\Documents\\GitHub\\team-beta\\images\\rf_data.csv")

# Remove Non-numeric Data
data = data %>% select(-X)

head(data)

```

```{R}
# Set the seed for reproducibility
set.seed(123)

data$Team.Success <- as.factor(data$Team.Success)

# Create stratified train-test split
train_index_new <- createDataPartition(data$Team.Success, p = 0.78, list = FALSE)

# Split the data into training and test sets
train_data_new <- data[train_index_new, ]
test_data_new <- data[-train_index_new, ]

```

```{R}
## Rename levels in Team.Success so train() can read them
levels(train_data_new$Team.Success)[levels(train_data_new$Team.Success) == "1"] <- "missed_po"
levels(train_data_new$Team.Success)[levels(train_data_new$Team.Success) == "2"] <- "made_po"
levels(train_data_new$Team.Success)[levels(train_data_new$Team.Success) == "3"] <- "runner_up"
levels(train_data_new$Team.Success)[levels(train_data_new$Team.Success) == "4"] <- "ws_winner"

```

```{R}
# Define trainControl with SMOTE
ctrl_new <- trainControl(
  method = "cv",           # k-fold cross-validation
  number = 10,              # 10 folds
  sampling = "smote", # apply SMOTE inside each fold
  classProbs = TRUE,
  savePredictions = "final"
)

```

```{R}
# Train KNN model
set.seed(500)
knn_model_new <- train(
  Team.Success ~ ., 
  data = train_data_new,
  method = "knn",
  trControl = ctrl_new,
  preProcess = c("center", "scale"),  # scale predictors before KNN
  tuneLength = 10                     # search over 10 different K values
)

```

```{R}
## Rename levels in Team.Success so they match training data
levels(test_data_new$Team.Success)[levels(test_data_new$Team.Success) == "1"] <- "missed_po"
levels(test_data_new$Team.Success)[levels(test_data_new$Team.Success) == "2"] <- "made_po"
levels(test_data_new$Team.Success)[levels(test_data_new$Team.Success) == "3"] <- "runner_up"
levels(test_data_new$Team.Success)[levels(test_data_new$Team.Success) == "4"] <- "ws_winner"
levels(test_data_new$Team.Success)

```

```{R}
# Scale numeric data of test set
test_x_new <- test_data_new %>% dplyr::select(-Team.Success)
test_x_new <- as.data.frame(scale(test_x_new))
test_y_new <- test_data_new$Team.Success

test_data_new <- test_x_new %>% mutate(Team.Success = test_y_new)

```

```{R}
# Predict test set response with trained cross-validated kNN
predictions_new <- predict(knn_model_new, newdata = test_data_new)

# Create detailed confusion matrix
confusionMatrix(predictions_new, test_data_new$Team.Success)

```

```{R}
# Creating a heatmap table for the confusion matrix
conf_matrix_new <- table(Predicted = predictions_new, Actual = test_data_new$Team.Success) # Create the table as a matrix
conf_df_new <- as.data.frame(conf_matrix_new) # Convert to data frame for ggplot

# Plot heatmap
ggplot(conf_df_new, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), size = 5, fontface = "bold") +
  scale_fill_gradient(low = "#deebf7", high = "#3182bd") +
  labs(title = "10-fold cross-validated kNN with LDA",
       subtitle = "Multiclass Team Success Response",
       x = "Team Success",
       y = "Predicted",
       fill = "Count") +
  theme_minimal(base_size = 14)

```

```{R}


```

```{R}


```

```{R}


```

```{R}


```

```{R}


```

```{R}


```
